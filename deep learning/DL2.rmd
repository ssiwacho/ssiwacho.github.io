---
title: "การเรียนรู้เชิงลึก"
output:
  rmdformats::robobook:
    self_contained: FALSE
---


```{r setup, include=FALSE}
library(knitr)
library(rmdformats)
library(reticulate)

## Global options
options(max.print="150")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=100)
```



<p style="line-height:1.3; font-size:16px; color: #353b48; text-align:right;">
สิวะโชติ ศรีสุทธิยากร
</br>
</br>
ภาควิชาวิจัยและจิตวิทยาการศึกษา คณะครุศาสตร์
</br>
จุฬาลงกรณ์มหาวิทยาลัย
</br>
</font> 13 May 2021 </font>
</p>




</body>
</html>



# **Introduction**

DL model กลุ่มของอัลกอริทึมการเรียนรู้ของเครื่องที่มีความสามารถสูงมากในปัจจุบัน โดยสามารถทำนายค่าของตัวแปรเป้าหมายได้อย่างมีประสิทธิภาพ และแม่นยำ DL ถูกนำไปประยุกต์ใช้ในหลากหลายวงการทั้งทางการแพทย์ เช่น การวินิจฉัยโรคจากภาพ X-ray หรือ CT Scan ทางการศึกษา เช่น การตรวจจับใบหน้าหรือเสียงของผู้เรียนเพื่อวินิจฉัยอารมณ์ความรู้สึกหรือความยึดมั่นผูกพันทางการเรียนของนักเรียน เป็นต้น

อัลกอริทึมหลักของ DL คือ Artificial neural network ซึ่งเป็นอัลกอริทึมที่มีจุดเด่นคือรองรับข้อมูลนำเข้าที่หลากหลายทั้งในรูปแบบข้อมูลที่มีโครงสร้าง และไม่มีโครงสร้าง แตกต่างจาก traditional ML algorithm ที่มักใช้ได้กับข้อมูลที่มีโครงสร้างเท่านั้น 

นอกจากนี้โครงสร้างการเรียนรู้ของ neural network ยังมีประสิทธิภาพและมีความยืดหยุ่น และมีลักษณะการเรียนรู้ที่สอดคล้องใกล้เคียงกับการเรียนรู้ของมนุษย์มากกว่า traditional ML รูปต่อไปนี้แสดงลักษณะการเรียนรู้แบบ traditional ML 

<left>![tradition ML algorithm](/Users/siwachoat/Library/Mobile Documents/com~apple~CloudDocs/github/ssiwacho/2758501/ssiwacho.github.io/deep learning/Screen Shot 2564-05-15 at 02.08.01.png){width=60%}</left>

*tradition ML algorithm*

จากรูปจะเห็นว่า traditional ML algorithm ประกอบด้วย 3 ส่วนหลักได้แก่ 

1. ส่วนข้อมูลนำเข้า (Input)

2. ส่วน Model

3. ส่วนการทำนาย

ส่วน neural network model มีลักษณะการเรียนรู้ดังรูป

<left>![](/Users/siwachoat/Library/Mobile Documents/com~apple~CloudDocs/github/ssiwacho/2758501/ssiwacho.github.io/deep learning/Screen Shot 2564-05-15 at 02.29.19.png){width=80%}</left>


จากรูปจะเห็นว่า neural network ประกอบด้วยส่วนประกอบหลักที่เรียกว่า layer จำนวน 3 ส่วนได้แก่ 

1. input layer 

2. hidden layer

3. output layer

จากรูปจะเห็นว่าโมเดลมี input layer ที่ประกอบด้วยหน่วยย่อยหรือที่เรียกว่า นิวรอน (neurons) จำนวน 3 หน่วย โดยนิวรอนใน input layer จะใช้แทนตัวแปรอิสระของโมเดล ในที่นี้จะได้ว่า $X = [x_1, x_2, x_3]$ 

โมเดลจะนำข้อมูลนำเข้าใน input layer ที่กำหนดมาประมวลผลร่วมกับพารามิเตอร์ $w[1]$ และ $b[1]$ เพื่อนำส่งไปยัง neuron แต่ละตัวภายใน hidden layer ผู้อ่านจะเห็นว่าการประมวลผลเพื่อให้ได้ neuron ดังกล่าวสามารถเขียนเป็นแผนภาพได้ดังนี้

<left>![](/Users/siwachoat/Library/Mobile Documents/com~apple~CloudDocs/github/ssiwacho/2758501/ssiwacho.github.io/deep learning/Screen Shot 2564-05-15 at 02.32.24.png){width=50%}</left>

เมื่อได้ค่าของ neuron แต่ละตัวในชั้นของ hidden layer จึงทำการประมวลผลร่วมกับพารามิเตอร์ $w[2]$ และ $b[2]$ เพื่อคำนวณค่าทำนายใน output layer เรียกกระบวนการนี้ว่า **forward propagation**


การประมวลผลของ neuron ใน hidden layer จะนำข้อมูลของ neuron ที่อยู่ใน layer ก่อนหน้ามาคำนวณผลรวมเชิงเส้นแบบถ่วงน้ำหนัก จากตัวอย่างของ deep learning model ในข้างต้น จะได้ว่าผลรวมเชิงเส้นของแต่ละ neuron ใน hidden layer คำนวณได้ดังนี้

$h_j=b+w_1x_1+w_2x_2$

เรียก $w_k$ ว่าน้ำหนัก (weight) และ $b$ ว่าค่าความลำเอียง (bias) 

โดยทั่วไปมีความเป็นไปได้ที่ค่าของผลรวมเชิงเส้นที่คำนวณได้ข้างต้นจะมีพิสัยที่ไม่สอดคล้องกับตัวแปรตามที่ต้องการทำนาย จึงมีการนำค่าผลรวมเชิงเส้นดังกล่าวมาแปลงค่าด้วยฟังก์ชันที่เรียกว่า activation function ก่อนที่จะส่งค่าไปยัง neuron ใน layer ถัดไป ดังตัวอย่างในรูปด้านล่าง


## **Forward propagation**

โดยปกติแล้วเมื่อเริ่มต้นกระบวนการผู้วิเคราะห์จะยังไม่ทราบค่าพารามิเตอร์ที่เหมาะสมของโมเดลดังนั้นจึงต้องมีการประมาณค่าพารามิเตอร์ดังกล่าวก่อนการใช้งานจริง กระบวนการ forward propagation จะเริ่มจากการสุ่มค่าเริ่มต้นของพารามิเตอร์ทั้งหมดภายในโมเดลขึ้นมาก่อน 1 ชุด และใช้ชุดของพารามิเตอร์นี้ไปใช้กับชุดข้อมูลฝึกหัดเพื่อคำนวณเป็นผลลัพธ์ใน output layer 

เมื่อได้ผลลัพธ์ดังกล่าวจะนำผลลัพธ์นี้ไปเปรียบเทียบกับค่าจริงของผลลัพธ์ในชุดข้อมูลฝึกหัดโดยใช้ cost function จากนั้นจะใช้กระบวนการ **backward propagation** เพื่อปรับค่าพารามิเตอร์ในโมเดลเพื่อให้ค่าของ loss function ลดลง และได้ชุดของพารามิเตอร์ที่มีความสมเหตุสมผลมากยิ่งขึ้น

กระบวนการทำงานข้างต้นจะดำเนินการแบบทวนซ้ำหลายรอบจนกระทั่งค่า cost function ของโมเดลลดลงอยู่ในระดับที่ต้องการ ในเชิงเทคนิคจะเรียกกระบวนการ forward + backward propagation 1 ชุด นี้ว่า 1 Epoch

## **Backward propagation**

กระบวนการ forward propagation จะสิ้นสุดลงเมื่อคำนวณค่าทำนายของตัวแปรเป้าหมายในโมเดลได้ เมื่อได้ค่าทำนายดังกล่าวผู้วิเคราะห์จะทำการประเมินความสอดคล้องเชิงประจักษ์ของโมเดลด้วยการเปรียบเทียบค่าทำนายนี้กับค่าจริงในชุดข้อมูลฝึกหัดผ่านฟังก์ชันวัตถุประสงค์หรือฟังก์ชันต้นทุน (cost function) ซึ่งมีหลากหลายตัวขึ้นกับปัญหาของการทำงาน เช่นในปัญหา regression อาจใช้ cost function เป็นค่า mean squares error (MSE) หรือปัญหา binary classification อาจใช้ cost function เป็น cross entropy ดังนี้

$J = -\frac{1}{n}\sum_{i=1}^n (y_ilog(\hat{y}_i)+(1-y_i)log(1-\hat{y}_i))$

กระบวนการ backward propagation จะปรับค่าพารามิเตอร์ในโมเดลเพื่อให้ได้โมเดลที่มีความสอดคล้่องกับข้อมูลฝึกหัดมากขึ้น โดยพิจารณาชุดของค่าพารามิเตอร์ที่ทำให้ cost function มีค่าต่ำที่สุด โดยใช้อัลกอริทึม gradient descent เป็นเครื่องมือ ดังนี้

1. หา first-order derivative ของ cost function เทียบกับค่าพารามิเตอร์ในโมเดลเขียนแทนด้วย $\frac{\partial J(w)}{\partial w}$

2. นำ $\frac{\partial J(w)}{\partial w}$ ไปหักลบออกจากค่าพารามิเตอร์ในรอบก่อนหน้าโดยควบคุมความเร็วในการเรียนรู้ผ่าน learning rate ($\alpha$)

3. นำค่าพารามิเตอร์ที่ปรับปรุงใหม่ใน 2 ไปประมวลผลร่วมในกระบวนการ forward propagation ซึ่งจะทำให้ cost function มีค่าลดลง

ทวนซ้ำกระบวนการข้างต้นจนกระทั่ง cost function มีแนวโน้มคงที่


## **Activation function**

activation function เป็นฟังก์ชันทางคณิตศาสตร์ ที่ผู้วิเคราะห์ใช้สำหรับแปลงค่าผลรวมเชิงเส้นของข้อมูลนำเข้า ให้มีพิสัยอยู่ในช่วงที่สมเหตุสมผลหรือสอดคล้องกับตัวแปรตามที่ต้องการทำนาย 

activation function ที่ใช้ในการวิเคราะห์มีได้หลากหลาย โดยอาจจำแนกเป็น 2 ประเภทได้แก่ **linear activation function** ซึ่งเขียนในรูปทั่วไปดังนี้

$\sigma(x)=m(x)+c$ 
 
ฟังก์ชันประเภทนี้จะไม่ได้ทำให้พิสัยของข้อมูลนำเข้าเปลี่ยนแปลงไปจากเดิม กล่าวคือหาก $x \in [-\infty, \infty]$ แล้ว $\sigma(x) \in [-\infty, \infty]$ เช่นเดิม แต่อาจช่วยแปลงสเกลของข้อมูลนำเข้าให้เหมาะสมมากหรือใกล้เคียงกับช่วงที่เป็นไปได้ของตัวแปรตามมากขึ้น ดังตัวอย่างในรูปด้านล่าง

```{r echo=F, fig.width=5, fig.height=4}
x<-seq(-10,10,1)
y<-1+0.2*x
par(mar=c(4,4,1,1))
plot(x,y, xlab="x", ylab=expression(sigma(x)),type="l")
```

และ **nonlinear activation function** ซึ่งมักใช้ใช้บ่อยกว่า linear activation function ทั้งนี้เป็นเพราะมีความสามารถที่จะแปลงพิสัยของข้อมูลนำเข้าให้อยู่ในช่วงที่เหมาะสมได้ เช่นแปลง $x \in [-\infty, \infty]$ ให้อยู่ในช่วง $[0,1]$ หรือ $[0, \infty]$ หรือ $[-1,1]$ 


### ตัวอย่าง activation function

**Sigmoid function:** $\sigma(x)=\frac{1}{1+exp(-x)}$

เหมาะสำหรับใช้เป็น activation function ในปัญหา binary classification ฟังก์ชันนี้จะแปลงข้อมูลนำเข้าในอยู่ในพิสัย $[0,1]$ 

**Hyperbolic Tangent function:** $\sigma(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

ฟังก์ชันนี้มีลักษณะคล้ายกับ sigmoid function แต่พิสัยของฟังก์ชันจะอยู่ในช่วง $[-1,1]$ บางครั้งเรียกว่า sigmoidal funcion หรือ tanh function

```{r echo=F, fig.width=4, fig.height=3, fig.align="left"}
x<-seq(-10,10,1)
y<-1/(1+exp(-x))
z<-(exp(x)-exp(-x))/(exp(x)+exp(-x))
par(mar=c(4,4,1,1))
plot(x,y, xlab="x", ylab="Sigmoid function",type="l", ylim=c(-1,1))
points(x,z, xlab="x", ylab="tanh function",type="l", lty=2)
legend(3,-0.5, lty=c(1,2), legend=c("Sigmoid","tanh"),cex=0.6)
abline(h=0, lty=3, col="grey")
abline(h=-1, lty=3, col="grey")
abline(h=1, lty=3, col="grey")
```

**Rectified Linear Unit:** $\sigma(x)=max(0,x)$

เป็น activation function ที่มักใช้บ่อยใน hidden layer โดยพิสัยของฟังก์ชันจะอยู่ในช่วง $[0,\infty]$ ดังรูป

```{r echo=F, fig.width=4, fig.height=3, fig.align="left"}
x<-seq(-10,10,1)
y<-ifelse(x>0,x,0)
par(mar=c(4,4,1,1))
plot(x,y, xlab="x", ylab="ReLU function",type="l")
grid(5,5)
```

**Leaky Rectified Linear Unit:** $\sigma(x)= \begin{cases} x,& \text{if } x\geq 1\\ax, & \text{otherwise}\end{cases}$

```{r echo=F, fig.width=4, fig.height=3, fig.align="left"}
x<-seq(-10,10,1)
y<-ifelse(x>0,x,0.05*x)
par(mar=c(4,4,1,1))
plot(x,y, xlab="x", ylab="ReLU function",type="l")
grid(5,5)
```

**Softmax** 

softmax activation function เป็น function ที่เหมาะสำหรับใช้ในปัญหา multi-class classification จากสูตรของฟังก์ชันจะเห็นว่าผลลัพธ์ที่ได้จะเป็นความน่าจะเป็นที่มีพิสัยอยู่ในช่วง $[0,1]$

กำหนดให้ $x=(x_1, x_2, x_3, x_4)$ เป็นเวกเตอร์ของข้อมูลนำเข้า และตัวแปรตามเป็นตัวแปรแบบจัดประเภทที่มี 3 ระดับได้แก่ $a, b, c$ 

สมมุติว่าต้องการหาความน่าจะเป็นของการเกิดผลลัพธ์ $a, b, c$ เมื่อกำหนดข้อมูลนำเข้า $x$ จะสามารถหาได้โดยใช้ความน่าจะเป็นแบบมีเงื่อนไข (conditional probability) ดังนี้


$P(a|x)=\frac{y_a}{y_a+y_b+y_c}$

$P(b|x)=\frac{y_b}{y_a+y_b+y_c}$

$P(c|x)=\frac{y_c}{y_a+y_b+y_c}$

เมื่อ

$y_a=w_{1,a}x_1+w_{2,a}x_2+w_{3,a}x_3+w_{4,a}x_4$

$y_b=w_{1,b}x_1+w_{2,b}x_2+w_{3,b}x_3+w_{4,b}x_4$

$y_c=w_{1,c}x_1+w_{2,c}x_2+w_{3,c}x_3+w_{4,c}x_4$

จะสังเกตเห็นว่า $P(a|x)+P(b|x)+P(c|x)=1$

เนื่องจากผลรวมเชิงเส้นในข้างต้นมีพิสัยเป็นจำนวนจริง จึงทำให้ผลรวมเชิงเส้นดังกล่าวสามารถมีค่าติดลบได้ ซึ่งอาจส่งผลให้ค่าความน่าจะเป็นแบบมีเงื่อนไขมีค่าอยู่นอกช่วง $[0,1]$ 

เพื่อแก้ปัญหานี้จึงมีการใช้ฟังก์ชัน exponential แปลงพิสัยของผลรวมเชิงเส้นที่เป็นจำนวนจริงให้อยู่ในช่วง $[0, \infty]$ ซึ่งทำให้ sofmax activation function สามารถเขียนในรูปทั่วไปได้ดังนี้

กำหนดให้ $x = (x_1, x_2, x_3, ..., x_p)^T$ เป็นเวกเตอร์ข้อมูลนำเข้าของตัวแปรอิสระจำนวน $p$ ตัว และ $y$ เป็นตัวแปรตามแบบจัดประเภทที่มีจำนวน $k$ ระดับ softmax activation function นิยามดังนี้


$\sigma(x)=P(y=y_m|x)=\frac{exp(x_i)}{\sum_{j=1}^pexp(x_j)}$

เมื่อ $i = 1,2,...,p$ และ $m = 1,2,...,k$


์Note: สังเกตว่า DL model ทำนายตัวแปรตามที่ต้องการด้วยการแยกส่วนการประมวลผลออกเป็นส่วนย่อย ๆ หลาย ๆ ส่วน โดยที่แต่ละส่วนถูกบรรจุอยู่ใน neuron


## **ประเภทของ ​DL model**

- Multilayer Perceptron Model (MLP models)

- Convolutional Neural Network (CNNs)

- Recurrent Neural Network (RNNs)

- Restricted Boltzmann Machines (RBMs)

- Deep Belief Networks (DBNs)


# **The first DL model (MLP) with Keras**

multi-layer perceptron เป็น neural network ประเภทหนึ่งที่เรียกว่า Feedforward neural networks (FFNNs) ซึ่งเป็นโมเดลพื้นฐานที่ใช้ในการทำงานทั่วไป ภายในโมเดลประกอบด้วย input, hidden และ output layers ดังที่กล่าวมาแล้ว โมเดลประเภทนี้สามารถประยุกต์ใช้ได้กับทั้งปัญหา classification และ regression

![](/Users/siwachoat/Library/Mobile Documents/com~apple~CloudDocs/github/ssiwacho/2758501/ssiwacho.github.io/deep learning/Screen Shot 2564-05-15 at 03.15.07.png)

*(a) Single layer perceptron; (b) multi-layer perceptron*

หัวข้อนี้จะกล่าวถึงการสร้าง MLP โดยใช้ package-keras 

<iframe width="560" height="315" src="https://www.youtube.com/embed/j_pJmXJwMLA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## ติดตั้ง package

keras สามารถติดตั้งและเรียกใช้งานได้ทั้งบน python และ R โดยสำหรับภาษา python ให้ดำเนินการดังนี้

1. ในหน้าต่าง terminal ให้ดำเนินการดาวน์โหลดและติดตั้ง tensorflow และ keras ดังนี้

```{python eval=F}
pip install --upgrade tensorflow
pip install keras
```

2. ใน python ก่อนการใช้งานแต่ละครั้งจะเป็นต้องเรียกใช้โดยพิมพ์คำสั่งดังนี้

```{python eval=F}
from keras.models import Sequential
from keras.layers import Dense
```

สำหรับโปรแกรม R ให้ดำเนินการดาวน์โหลดและติดตั้ง package-keras โดยพิมพ์คำสั่งดังนี้

```{r eval=F}
install.packages("keras")
install_keras()
library(keras)
```

## Regression Example

จากชุดข้อมูล crimm.csv สมมุติว่่าต้องการสร้าง MLP เพื่อทำนายอัตราการเกิดคดีโจรกรรมของแต่ละพื้นที่ 

ใน python สามารถดำเนินการได้ดังนี้ ---> [MLP in Python](https://ssiwacho.github.io/deep%20learning/DL_reg_1.html)

ใน R สามารถดำเนินการได้ดังนี้

1. นำเข้าชุดข้อมูล 

```{r}
library(dplyr)
library(caret)
dat<-read.csv("/Users/siwachoat/Library/Mobile Documents/com~apple~CloudDocs/เอกสารประกอบการสอน/Machine Learning/เอกสาร/ep2_regression/_02_multiple regression/crimm.csv")
dat<-dat[,-1]
glimpse(dat)
```

2. normalized ค่าของตัวแปรอิสระ

```{r}
dat[,1:16]<-scale(dat[,1:16], center=TRUE, scale=TRUE)
summary(dat)
```

3. แบ่งชุดข้อมูลเป็น train and test data โดยให้เก็บค่าไว้ในรูปแบบของเมทริกซ์ (matrix) จำแนกเป็นเมทริกซ์ของตัวแปรอิสระ และเมทริกซ์ของตัวแปรตาม 

```{r}
train.id<-createDataPartition(dat$TheifperPop, p = 0.8 , list=F)
train_x<-as.matrix(dat[train.id, 1:16])
train_y<-as.numeric(dat[train.id, 17])

test_x<-as.matrix(dat[-train.id, 1:16])
test_y<-as.numeric(dat[-train.id, 17])

#remove column name
dimnames(train_x)<-NULL
dimnames(train_y)<-NULL
```

4. สร้างโมเดล MLP ด้วย package-keras

```{r eval=F}
#1. Create a Sequential model
model.r <- keras_model_sequential()

#2. Add input, hidden and output layer
model.r %>% layer_dense(units = 32, input_shape=c(16), activation="relu") %>%
            layer_dense(units=16, activation="relu")%>%
            layer_dense(units = 1)

#3. summary the model
model.r

#4. compile model
model.r %>% compile(optimizer = "adam", loss = "mse")

#5. train the model
history <- model.r %>% keras::fit(train_x, train_y, epoch = 500)
```

```{r echo=F}
#1. Create a Sequential model
model.r <- keras_model_sequential()

#2. Add input, hidden and output layer
model.r %>% layer_dense(units = 32, input_shape=c(16), activation="relu") %>%
            layer_dense(units=16, activation="relu")%>%
            layer_dense(units = 1)

#3. summary the model
model.r

#4. compile model
model.r %>% compile(optimizer = "adam", loss = "mse")

#5. train the model
history <- model.r %>% keras::fit(train_x, train_y, epoch = 500, verbose=0, validation_split=0.2)

#6. Evaluate the model
mse <-model.r %>% evaluate (test_x, test_y)
pred<-model.r%>%predict(test_x)
```

5. ประเมินคุณภาพของโมเดล

```{r}
plot(history)
mse #MSE
sqrt(mse) #RMSE
par(mar=c(4,4,1,1))
plot(pred, test_y)
```





## Classification Example









<left>![](/Users/siwachoat/Library/Mobile Documents/com~apple~CloudDocs/github/ssiwacho/2758501/ssiwacho.github.io/deep learning/Screen Shot 2564-05-13 at 13.44.05.png){width=40%}</left>



