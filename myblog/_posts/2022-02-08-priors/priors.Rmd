---
title: "Prior Distributions"
categories:
  - Bayesian Statistics
  - priors
description: |
  A short description of the post.
author:
  - name: Siwachoat Srisuttiyakorn
    url: 
date: 2022-02-07
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


การแจกแจงความน่าจะเป็นก่อนหน้า (prior distribution) เป็นส่วนประกอบสำตัญในการวิเคราะห์ข้อมูลด้วยสถิติแบบเบส์ ทั้งนี้เป็นเพราะการแจกแจงความน่าจะเป็นก่อนหน้าทำให้ผู้วิเคราะห์สามารถสร้าง statement ความน่าจะเป็นของพารามิเตอร์ภายในโมเดลสถิติที่ใช้ในการวิเคราะห์ข้อมูลได้  นอกจากนี้การแจกแจงความน่าจะเป็นก่อนหน้ายังมีส่วนช่วยเพิ่มเติมสารสนเทศจากแหล่งอื่น ๆ นอกเหนือจากข้อมูลเชิงประจักษ์ให้กับผลการวิเคราะห์ได้อีกด้วย บทเรียนนี้จะกล่าวถึงมโนทัศน์และการกำหนดการแจกแจงความน่าจะเป็นก่อนหน้าในโมเดลการวิเคราะห์ รายละเอียดมีดังนี้


## ประเภทของ priors

โดยทั่วไปแล้วอาจจำแนกการแจกแจงความน่าจะเป็นก่อนหน้าออกได้เป็น 2 ประเภท ได้แก่ การแจกแจงความน่าจะเป็นก่อนหน้าแบบไม่ให้สารสนเทศ (noninformative priors) และการแจกแจงความน่าจะเป็นก่อนหน้าแบบให้สารสนเทศ (informative priors)

ในกรณีที่ผู้วิเคราะห์เลือกใช้การแจกแจงความน่าจะเป็นก่อนหน้าแบบไม่ให้สารสนเทศ ผลการวิเคราะห์ที่ได้ในสถานการณ์นี้จะมีความเป็นปรนัย (objective) ทั้งนี้เป็นเพราะ noninformative priors เป็นการแจกแจงที่ไม่ได้ให้สารสนเทศใด ๆ เกี่ยวกับพารามิเตอร์ในโมเดลเพิ่มเติม ทำให้ผลการวิเคราะห์ต่าง ๆ ทั้งการประมาณค่า การทำนาย การเปรียบเทียบโมเดล จะขึ้นกับสารสนเทศที่ได้จากข้อมูลตัวอย่างเท่านั้น การที่ผู้วิเคราะห์เลือกใช้การแจกแจงความน่าจะเป็นก่อนหน้าประเภทนี้ในการวิเคราะห์ แสดงว่าผู้วิเคราะห์ต้องการให้ผลการวิเคราะห์ที่ได้มีความเป็นปรนัยมากที่สุด 

หนังสือหลายเล่มใช้คำว่า vague หรือ diffuse priors แทนคำว่า noninformative priors เพราะต้องการหลีกเลี่ยงการเข้าใจผิดว่า noninformative priors ไม่มีสารสนเทศ แต่ในความเป็นจริงการแจกแจงทุกตัวนั้นมีสารสนเทศเสมอ เพียงแต่ว่าเป็นการแจกแจงที่มีการกระจายค่อนข้างมากจึงไม่ได้ช่วยให้การวิเคราะห์มีสารสนเทศเพิ่มเติมไปจากสารสนเทศในข้อมูลตัวอย่าง

ในทางกลับกันหากผู้วิเคราะห์เลือกใช้ informative priors ในการวิเคราะห์ นั่นหมายความว่าผู้วิเคราะห์ดำเนินการวิเคราะห์โดยไม่ได้ใช้แต่สารสนเทศที่อยู่ภายในข้อมูลตัวอย่างเท่านั้น แต่ยังใช้สารสนเทศที่อยู่ภายในการแจกแจงความน่าจะเป็นก่อนหน้ามาร่วมประมวลผลด้วย ผลการวิเคราะห์ที่ได้ในกรณีนี้จะมีความเป็นอัตวิสัยมากกว่า


## Noninformative priors

การแจกแจงความน่าจะเป็นก่อนหน้าแบบไม่ให้สารสนเทศในโมเดลมักใช้ในกรณีที่ผู้วิเคราะห์ไม่ได้มีความรู้หรือสารสนเทศเพิ่มเติมเกี่ยวกับพารามิเตอร์ต่าง ๆ ภายในโมเดลการวิเคราะห์ หรืออีกกรณีคือใช้ผลการวิเคราะห์ดังกล่าวเป็นผลการวิเคราะห์อ้างอิง (reference output) สำหรับการวิเคราะห์ผลกระทบที่เกิดจากการกำหนด informative priors หัวข้อนี้จะกล่าวถึงการแจกแจงความน่าจะเป็นก่อนหน้าแบบไม่ให้สารสนเทศบางตัวที่มักใช้เป็นการแจกแจงมาตรฐานในโปรแกรม JAGs รายละเอียดมีดังนี้

### Uniform Distribution

การแจกแจงแบบสม่ำเสมอ (uniform) เป็นการแจกแจงหนึ่งที่นิยมใช้เป็นการแจกแจงความน่าจะเป็นก่อนหน้าแบบไม่ให้สารสนเทศการแจกแจงหนึ่ง ซึ่งมีลักษณะเด่นคือ เป็นการแจกแจงที่ให้น้ำหนักกับความเป็นไปได้ต่าง ๆ ของค่าพารามิเตอร์ที่เท่าเทียมกัน กล่าวคือเป็น prior ที่มีฟังก์ชันความน่าจะเป็นอยู่ในรูปแบบ

$p(\theta)=Const.$ เมื่อ $\theta \in A$ โดยที่ $A$ คือช่วงที่เป็นไปได้ของพารามิเตอร์ $\theta$

จากตัวอย่างการวิเคราะห์ความเที่ยงตรงของเหรียญ หากกำหนดการแจกแจงความน่าจะเป็นก่อนหน้าของพารามิเตอร์ความลำเอียงเป็น uniform(0,1) จะได้ $p(\theta)=1$ เมื่อ $\theta \in [0,1]$ รูปต่อไปนี้แสดงฟังก์ชันความน่าจะเป็นแบบ uniform บนช่วง 0,1

```{r echo=F}
plot(seq(0,1,0.01), dunif(seq(0,1,0.01),0,1), type="l", col="#004D80",
     xlab=expression(theta), ylab="Density")
```

จากรูปการกำหนดการแจกแจงความน่าจะเป็นก่อนหน้าในข้างต้นไม่ได้ให้น้ำหนักกับค่าหรือช่วงใดช่วงหนึ่งของพารามิเตอร์ความลำเอียงเป็นพิเศษ ซึ่งทำให้การแจกแจงความน่าจะเป็นภายหลังของพารามิเตอร์ความลำเอียงถูกพัฒนาขึ้นโดยใช้สารสนเทศจากฟังก์ชันภาวะความควรจะเป็นเกือบทั้งหมด ดังจะเห็นจากสมการด้านล่าง

$p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)} =\frac{p(D|\theta) \times 1}{P(D)}\varpropto p(D|\theta)$

ข้อควรระวังในการใช้การแจกแจงแบบ uniform เป็นการแจกแจงความน่าจะเป็นก่อนหน้าคือ การแจกแจงแบบ uniform ไม่มีคุณสมบัติ invariant กล่าวคือ ถ้า $\theta \sim Uniform(a,b)$ แต่ $\theta^2$ ไม่ได้มีการแจกแจงแบบ $Uniform(a,b)$ ดังกล่าว


### Jeffrey's prior

จากปัญหาข้างต้น Harold Jeffrey (1961) ได้เสนอการแจกแจงความน่าจะเป็นก่อนหน้าแบบไม่ให้สารสนเทศที่มีคุณสมบัติ invariant ชื่อว่า Jeffrey's prior ฟังก์ชันความน่าจะเป็นของการแจกแจงนี้แปรผันตรงกับรากที่สองของ determinant ของเมทริกซ์สารสนเทศของ Fisher (Fisher information matrix) ดังนี้

$p(\theta) \varpropto |J(\theta)|^{1/2}$

เมื่อ $J(\theta)$ คือ Fisher information matrix ที่คำนวณได้จาก

$J(\theta)=Var(\frac{\partial}{\partial \theta}ln \ p(\theta|y))$

$=E[-\frac{\partial^2 ln p(y|\theta)}{\partial \theta^2}|\theta]$

จะเห็นว่า Fisher information ในข้างต้นเป็นดัชนีที่ใช้วัดสารสนเทศที่ข้อมูลตัวอย่างมีเกี่ยวกับพารามิเตอร์ต่าง ๆ ภายในโมเดล จากสูตรจะเห็นว่าดัชนีดังกล่าวคำนวณค่าสารสนเทศดังกล่าวโดยอิงกับค่าความแปรปรวน เนื่องจากในกรณีทั่วไปโมเดลการวิเคราะห์สามารถมีพารามิเตอร์ได้มากกว่าหนึ่งตัว ทำให้ Fisher information ดังกล่าวมีลักษณะเป็นเมทริกซ์ความแปรปรวนร่วม (covariance matrix) การสรุปสารสนเทศจากเมทริกซ์ดังกล่าวในเชิงของความแปรปรวน วิธีการหนึ่งคือการหาค่าความแปรปรวนทั่วไป (generalized variance) ด้วยการหา determinant ของเมทริกซ์ความแปรปรวนร่วมดังกล่าว

จากมโนทัศน์ดังกล่าวจะเห็นว่า Jeffrey's prior เป็นการแจกแจงความน่าจะเป็นก่อนหน้าที่อิงกับการแจกแจงความน่าจะเป็นของตัวอย่างสุ่มเป็นหลัก จึงจัดเป็น noninformative prior ตัวหนึ่งสำหรับโมเดลการวิเคราะห์แบบพาราเมทริกซ์

ตัวอย่างต่อไปนี้แสดงการแจกแจงความน่าจะเป็นก่อนหน้าแบบ Jeffrey สำหรับปัญหาการวิเคราะห์ความเที่ยงของของเหรียญ

จากตัวอย่างการวิเคราะห์ความเที่ยงตรงของเหรียญ เนื่องจากฟังก์ชันภาวะความควรจะเป็นของพารามิเตอร์ความลำเอียงเมื่อกำหนดข้อมูลตัวอย่างคือ $p(D|\theta)=\theta^{\sum y_i}(1-\theta)^{n-\sum y_i}$ ดังนั้น log ของฟังก์ชันภาวะความควรจะเป็นดังกล่าวคือ
 
$ln \ p(D|theta) = \sum y_i ln\theta+(n-\sum y_i)ln(1-\theta)$

และอนุพันธ์อันดับที่ 2 ของ $ln \ p(D|theta)$ มีค่าเท่ากับ

$\frac{\partial^2 ln\ p(D|\theta)}{\partial\theta^2}=-\frac{\sum y_i}{\theta^2}-\frac{n-\sum y_i}{(1-\theta)^2}$


ดังนั้น Fisher Information ของพารามิเตอร์ $\theta$ ในโมเดลข้างต้นจึงมีค่าเท่ากับ

$J(\theta)=E[-\frac{\partial^2 ln p(y|\theta)}{\partial \theta^2}|\theta]=E[-(-\frac{\sum y_i}{\theta^2}-\frac{n-\sum y_i}{(1-\theta)^2})]=\frac{n}{\theta}+\frac{n(1-\theta)}{(1-\theta)^2}$


สมมุติว่าจากการเก็บรวบรวมข้อมูลพบว่า โยนเหรียญ 20 ครั้ง มีหน้าหัวเกิดขึ้น 7 ครั้ง แล้ว Jeffrey's prior ของพารามิเตอร์ความลำเอียงคือ

$p(\theta)=|J(\theta)|^{1/2}=|\frac{20}{\theta}+\frac{20(1-\theta)}{(1-\theta)^2}|^{-1/2}$

```{r fig.asp=0.6, echo=TRUE}
par(mar=c(5,5,1,1))
theta<-seq(0.01,0.99,0.01)
p.theta<-abs((20/theta+(20*(1-theta))/(1-theta)^2))^{0.5}
plot(theta, p.theta, type="l", col="#004D80",xlab=expression(theta), ylab="Density")
```

หมายเหตุ ในปัญหาที่ใช้ฟังก์ชันภาวะความควรจะเป็นแบบ binomial จะได้ว่า Jeffrey's prior จะมีการแจกแจงเทียบเท่ากับการแจกแจง beta ที่มีพารามิเตอร์ a = 0.5 และ b = 0.5


ตัวอย่างต่อไปนี้แสดงการเปรียบเทียบการแจกแจงความน่าจะเป็นภายหลังของพารามิเตอร์ความลำเอียง เมื่อกำหนดการแจกแจงความน่าจะเป็นก่อนหน้าเป็นแบบ uniform(0,1) และเป็นแบบ Jeffrey's prior

```{r message=F, warning=F}
library(runjags)

coin<-"model{

#likelihood function (model1)
for(i in 1:n)
{
y1[i]~dbern(theta1)
}

#likelihood function (model2)
for(i in 1:n)
{
y2[i]~dbern(theta2)
}

#prior distribution
theta1~dunif(0,1)
theta2~dbeta(0.5,0.5)
}
"

data.list<-list(y1=c(rep(1,7),rep(0,13)), 
                y2=c(rep(1,7),rep(0,13)), 
                n=20)
fit<-run.jags(model=coin,
              data=data.list,
              monitor = c("theta1","theta2"),
              n.chains=3,
              sample=5000,
              plots=F,
              summarise=T)

library(MCMCvis)
library(coda)
sample<-as.mcmc.list(fit)
sample1<-sample[,1]
sample2<-sample[,2]
prior1<-runif(10000,0,1)
prior2<-rbeta(10000,0.5,0.5)
prior<-cbind(prior1, prior2)
MCMCtrace(sample, pdf=F,
          params=c("theta1","theta2"),
          prior=prior,
          main_tr=c("trace-uniform prior","trace-Jeffrey's prior"),
          main_den=c("density-uniform prior","density-Jeffrey's prior"))

summary(sample)
```



คุณสมบัติเด่นของ Jeffrey priors คือมีคุณสมบัติ invariant to transformation กล่าวคือ หาก $\psi=f(\theta)$ จะได้ว่า $p(\psi) \varpropto |J(\psi)|^{0.5}$


พารามิเตอร์ในโมเดลการวิเคราะห์อาจจำแนกได้หลากหลายประเภท ขึ้นอยู่กับโมเดลการวิเคราะห์และตัวแปรในการวิเคราะห์ ทั้งนี้ธรรมชาติของพารามิเตอร์ดังกล่าวก็มีความแตกต่างกัน การกำหนดการแจกแจงความน่าจะเป็นก่อนหน้าของพารามิเตอร์แต่ละประเภทจึงมีความแตกต่างกันตามธรรมชาติดังกล่าว  โดยทั่วไปอาจจำแนกประเภทของพารามิเตอร์ได้ออกเป็น 4 ประเภทได้แก่ พารามิเตอร์ตำแหน่ง (location parameters) พารามิเตอร์สเกล (scale parameters) พารามิเตอร์สัดส่วน (proportion parameters) และพารามิเตอร์อัตรา (rate parameters) เนื้อหาต่อจากนี้จะกล่าวถึงการกำหนด informative priors สำหรับพารามิเตอร์แต่ละประเภท 


### Location parameters

กำหนดให้ $\theta$ เป็นพารามิเตอร์ของโมเดลค่าสังเกต $p(y|\theta)$ หากการแจกแจงของ $y-\theta$ เป็นอิสระจากพารามิเตอร์ $\theta$ จะเรียกว่าพารามิเตอร์ $\theta$ เป็นพารามิเตอร์ตำแหน่ง (location parameters) 








### Conjugacy priors

การแจกแจงความน่าจะเป็นก่อนหน้าจะเป็นการแจกแจงแบบ conjugacy prior ถ้าการแจกแจงความน่าจะเป็นก่อนหน้าและภายหลังเป็นการแจกแจงภายใน family เดียวกัน ด้วยคุณสมบัติดังกล่าวจึงทำให้การพิสูจน์หาสูตรหรือรูปแบบปิดของการแจกแจงความน่าจะเป็นภายหลังสามารถทำได้โดยที่ไม่ต้องหาเทอม $p(D)$ ที่เป็นตัวส่วนใน Bayes's rule 

ตัวอย่างการแจกแจงก่อนหน้าแบบ conjugacy prior เช่น

- การแจกแจง beta คู่กับ binomial likelihood จะได้ posterior แบบ beta

- การแจกแจง normal คู่กับ normal likelihood จะได้ posterior แบบ normal

<iframe width="560" height="315" src="https://www.youtube.com/embed/aPNrhR0dFi8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


### Informative priors

informative prior หรือการแจกแจงความน่าจะเป็นก่อนหน้าแบบมีสารสนเทศ เป็นการแจกแจงความน่าจะเป็นก่อนหน้าที่รูปแบบของการแจกแจงไม่จำเป็นต้องขึ้นกับหรือเกี่ยวข้องกับฟังก์ชันภาวะความควรจะเป็น แต่ขึ้นกับสารสนเทศจากแหล่งอื่น ๆ นอกเหนือจากข้อมูลจริงที่ผู้วิเคราะห์มีเกี่ยวกับพารามิเตอร์ต่าง ๆ ภายในโมเดล สารสนเทศภายในการแจกแจงความน่าจะเป็นก่อนหน้าประเภทนี้อาจเรียกว่า ความรู้ก่อนหน้า (prior knowledge) 

การกำหนดการแจกแจงความน่าจะเป็นก่อนหน้าลักษณะนี้จึงมีผลโดยตรงต่อการแจกแจงความน่าจะเป็นภายหลังของพารามิเตอร์ ซึ่งอาจทำให้ผลการรวิเคราะห์ที่ได้มีความถูกต้องมากขึ้น หรืออาจทำให้ผลการวิเคราะห์ที่ได้มีความลำเอียงก็ได้ ขึ้นอยู่กับการออกแบบ informative prior ของผู้วิเคราะห์


---
Distill is a publication format for scientific and technical writing, native to the web.

Learn more about using Distill at <https://rstudio.github.io/distill>.


